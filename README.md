Project Overview

This project aims to develop a mobile application that leverages Vision-Language Models (VLM) to assist individuals with visual impairments. The application captures images or videos, processes them to generate textual descriptions of the surroundings, and assists users by answering their queries related to the environment. Additionally, it can raise alarms in hazardous situations and maintain a log of interactions for caregivers to review.

Key Features

1. **Scene Description**: The app captures images or videos and converts them to text, describing colors and objects in the scene to users who are partially blind.
2. **Hazard Detection**: The app analyzes the surroundings and raises alarms in case of hazardous situations.
3. **Conversational Assistance**: Users can interact with a conversational bot that provides detailed descriptions of their surroundings and answers related queries.
4. **Interaction Logging**: The app maintains a log of all conversations and activities for caregivers to review the history of the user's surroundings.

Functional Requirements

1. **Image/Video Capture**: The mobile app should be able to capture pictures or videos.
2. **VLM Processing**: Utilize pre-trained Vision-Language Models to process the captured media and generate textual descriptions.
3. **User Queries**: Assist users by answering their questions about the surroundings based on the processed descriptions.
4. **Alarm System**: Automatically raise an alarm if hazardous surroundings are detected.
5. **Conversation Log**: Keep a detailed log of all interactions and activities for future reference by caregivers.

This project aims to significantly enhance the independence and safety of visually impaired individuals by providing real-time, detailed descriptions of their environment and alerting them to potential dangers.


